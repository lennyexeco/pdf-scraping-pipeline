# Web Scraping Pipeline V2

A serverless, AI-enhanced web scraping pipeline for multiple customers and projects, built on Google Cloud Platform. This pipeline dynamically analyzes website structures, extracts content and metadata, processes it, and generates structured outputs (XML, CSV reports).

## Setup

1.  **Clone Repository:** Get the latest code.
2.  **Google Cloud Project:**
    * Ensure you have a GCP project set up.
    * Enable necessary APIs: Cloud Functions, Pub/Sub, Firestore, Cloud Storage, Secret Manager, Vertex AI, Cloud Logging, Cloud Build.
    * Create a Service Account with appropriate roles (e.g., Cloud Functions Invoker, Pub/Sub Publisher/Subscriber, Storage Object Admin, Firestore User, Secret Manager Secret Accessor, Vertex AI User, Logs Writer).
3.  **Local Authentication & Configuration:**
    * Authenticate `gcloud` CLI: `gcloud auth login`
    * Authenticate Application Default Credentials (ADC) for local testing/scripting: `gcloud auth application-default login`
    * Set ADC quota project: `gcloud auth application-default set-quota-project YOUR_GCP_PROJECT_ID`
    * Configure Apify API Key (if used by any part of your data sourcing before this pipeline): Store it in Google Cloud Secret Manager. The project configuration files will reference the secret ID.
4.  **Customer and Project Configurations:**
    * Set up initial customer configs in `src/configs/customers/`.
    * Set up initial static project configs in `src/configs/projects/`. (See "Adding a New Project" for details).
5.  **Deploy Infrastructure & Functions:**
    * Run the deployment script: `./deploy.sh`. This script (from your provided files) creates Pub/Sub topics and deploys all Cloud Functions.

## Pipeline Structure

-   `src/functions/`: Contains the Python code for individual Cloud Functions, each representing a pipeline stage.
-   `src/configs/`:
    -   `customers/`: JSON configurations for each customer (GCP project ID, default Firestore DB).
    -   `projects/`: JSON configurations for each project/website (GCS buckets, Firestore collections, static fallbacks for selectors, field mappings, XML/report structures). These are augmented at runtime by AI-generated schemas.
-   `src/common/`: Shared Python utilities, including the dynamic configuration loader (`config.py`), helper functions (`helpers.py`), and logging setup (`utils.py`).
-   `src/inputs/`: Example input CSVs can be stored here for reference.
-   `scripts/` (or root): Contains the `deploy.sh` script.
-   `logs/`: Stores deployment logs generated by `deploy.sh`.

## Core Processing Steps & Data Flow

The pipeline is event-driven, primarily using Pub/Sub to trigger subsequent functions.

1.  **AI Schema Analysis (`analyze_website_schema`):**
    * **Trigger:** Manual Pub/Sub message to `start-analyze-website-schema-topic` (for new sites).
    * **Input:** Customer ID, Project Config Name, GCS path to a CSV of category URLs.
    * **Action:** Samples URLs, fetches HTML, uses Vertex AI to propose a site-specific schema (`metadata_fields`, `field_mappings`, `xml_structure`, `discovery_selectors`, etc.).
    * **Output:** Saves the AI-generated schema to Firestore (in `projects/{project_config_name}` document) and GCS.
    * **Next:** Automatically triggers `ingest_category_urls`.

2.  **Category URL Ingestion (`ingest_category_urls`):**
    * **Trigger:** Pub/Sub message on `start-ingest-category-urls-topic` (from `analyze_website_schema` or manual).
    * **Input:** Customer ID, Project Config Name, GCS path to CSV of all category URLs.
    * **Action:** Reads all category URLs from the CSV.
    * **Next:** For each category URL, publishes a message to `discover-main-urls-topic`.

3.  **Main URL Discovery (`discover_main_urls`):**
    * **Trigger:** Pub/Sub message on `discover-main-urls-topic`.
    * **Input:** Customer ID, Project Config Name, Category URL.
    * **Action:** Loads dynamic project config (static + AI schema). Fetches category page. Uses an AI-enhanced strategy (live AI analysis for first page, then dynamic config selectors, then static fallbacks) to find main content URLs and pagination.
    * **Next:** For each unique main URL, publishes to `extract-initial-metadata-topic`.

4.  **Initial Metadata Extraction (`extract_initial_metadata`):**
    * **Trigger:** Pub/Sub message on `extract-initial-metadata-topic`.
    * **Input:** Customer ID, Project Config Name, Main URL, Category URL.
    * **Action:** Loads dynamic project config. Scrapes the main URL. Uses Vertex AI (Gemini) to extract metadata based on `metadata_extraction_fields_config` (from AI schema). Generates `Law-ID` if configured.
    * **Output:** Stores item metadata (identifier, URLs, AI-extracted fields, `Law-ID`) in Firestore (e.g., `{project_firestore_collection}/{url_hash}`).
    * **Next:** Publishes to `fetch-content-topic`.

5.  **Content Fetching & Processing (`fetch_content`):**
    * **Trigger:** Pub/Sub message on `fetch-content-topic`.
    * **Input:** Customer ID, Project Config Name, Item Identifier, Main URL.
    * **Action:** Loads dynamic project config. Fetches full HTML of the main URL. Absolutizes internal links. Discovers, downloads, and stores linked PDFs.
    * **Output:** Stores processed HTML to GCS (e.g., `processed_html/{project_name}/{date}/{identifier}/{filename}.html`). Stores PDFs to GCS. Updates Firestore with GCS paths.
    * **Next:** Publishes to `generate-xml-topic`.

6.  **XML Generation (`generate_xml`):**
    * **Trigger:** Pub/Sub message on `generate-xml-topic`.
    * **Input:** Customer ID, Project Config Name, Item Identifier.
    * **Action:** Loads dynamic project config. Fetches item data from Firestore and its processed HTML from GCS. Builds an XML document based on `xml_structure` and `field_mappings`.
    * **Output:** Stores XML file in GCS (e.g., `delivered_xml/{project_name}/{date}/{filename}.xml`). Updates Firestore with XML path and status.
    * **Next:** Optionally triggers `generate_reports`.

7.  **Report Generation (`generate_reports`):**
    * **Trigger:** Pub/Sub message on `generate-reports-topic`.
    * **Input:** Customer ID, Project Config Name, Report Date.
    * **Action:** Loads dynamic project config. Queries Firestore for items processed on the report date.
    * **Output:** Generates a CSV report and stores it in GCS (e.g., `reports/{project_name}/{date}/report.csv`).

8.  **Retry Pipeline (`retry_pipeline`):**
    * **Trigger:** Messages published by any failing function to the `retry-pipeline` topic.
    * **Action:** Loads dynamic project config. Analyzes the error using Vertex AI, suggests retry strategies, and can re-trigger the failed stage with original or adjusted parameters. Logs attempts in Firestore.

## Deploying Cloud Functions

-   The `deploy.sh` script handles the deployment of all Cloud Functions listed in it.
-   It creates staging directories for each function, copies necessary shared code (`src/common`, `src/configs`), and then runs `gcloud functions deploy`.
-   Deployment logs are saved to the `logs/` directory in your project root.
-   Ensure each function's `requirements.txt` includes all necessary dependencies (e.g., `google-cloud-firestore`, `google-cloud-storage`, `google-cloud-pubsub`, `google-cloud-aiplatform`, `requests`, `beautifulsoup4`). The `deploy.sh` script attempts to check for some common ones.
-   Functions are deployed as 1st Gen (`--no-gen2`) by default in the provided script.

## Adding a New Project (Website/Source)

Follow these steps to onboard a new project (e.g., `example_new_site`):

1.  **Create/Update Customer Configuration:**
    * If it's a new customer, create `src/configs/customers/your_customer_id.json`.
        ```json
        {
          "gcp_project_id": "YOUR_GCP_PROJECT_ID",
          "firestore_database_id": "YOUR_FIRESTORE_DATABASE_ID_OR_(default)"
        }
        ```

2.  **Create Project-Specific Configuration:**
    * Create `src/configs/projects/example_new_site.json`.
    * Populate this file with essential details like `customer_id`, `gcs_bucket`, `firestore_collection` (choose a unique name), `firestore_database_id`, and provide **basic static fallbacks** for `discovery_fallback_selectors`, `field_mappings`, `xml_structure`, and `report_config`. Also, set the target website's `base_url` in `image_url_fix`.
    * Add new configuration fields like `title_field_in_firestore_for_filename`, timeouts, and GCS path templates.
    * *(Refer to the detailed template provided in the previous guide for a comprehensive example.)*

3.  **Prepare Initial Category URLs CSV:**
    * Create a CSV file listing all starting category/search URLs for `example_new_site`.
    * Upload this CSV to GCS, e.g., `gs://<your-gcs-bucket>/example_new_site/inputs/category_urls.csv`.

4.  **Deploy Functions (If shared code changed):**
    * If you modified any code in `src/common/` or the function deployment logic, re-run `./deploy.sh`. Config-only changes typically don't require a redeploy.

5.  **Trigger the Pipeline for the New Project:**
    * **Option A (Recommended for new sites - With AI Schema Analysis):**
        * Publish a message to `start-analyze-website-schema-topic`:
            ```json
            {
              "customer": "your_customer_id",
              "project": "example_new_site",
              "csv_gcs_path": "gs://<your-gcs-bucket>/example_new_site/inputs/category_urls.csv"
            }
            ```
        * This will generate and save the AI schema, then automatically start the ingestion process.
    * **Option B (Direct Ingestion - If schema exists or using only static config):**
        * Publish a message to `start-ingest-category-urls-topic` with the same payload as Option A.

## Monitoring

-   **Cloud Logging:** Primary source for function logs and error details. Filter by function name.
-   **Firestore:**
    * `projects/{project_config_name}`: Check for the `metadata_schema` field after `analyze_website_schema` runs.
    * `{project_firestore_collection}/{url_hash}`: (e.g., `example_new_site_data/{url_hash}`) View processed item data.
    * `{project_firestore_collection}_errors/{url_hash}`: View error details and retry history.
-   **Google Cloud Storage:**
    * Inspect your GCS bucket for:
        * Input CSVs: e.g., `<project_config_name>/inputs/`
        * AI Schema Backups: e.g., `schemas/<project_config_name>/`
        * Processed HTML: e.g., `processed_html/<project_config_name>/...`
        * Downloaded PDFs: e.g., `processed_pdfs/<project_config_name>/...`
        * Generated XMLs: e.g., `delivered_xml/<project_config_name>/...`
        * Reports: e.g., `reports/<project_config_name>/...`